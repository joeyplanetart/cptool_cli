"""URL 404æ£€æµ‹å‘½ä»¤å®ç°"""
import click
import asyncio
import csv
import random
import shutil
import webbrowser
from pathlib import Path
from urllib.parse import urlparse, urljoin
from datetime import datetime
from typing import List, Dict
import sys

from playwright.async_api import async_playwright, Browser
from cptools.utils.logger import setup_logger
from cptools.utils.url404_report import generate_url404_html_report
from cptools.utils.dingding import send_dingding_notification


@click.command()
@click.option(
    '--host', '-h', required=True,
    help='é»˜è®¤ä¸»æœºåœ°å€ï¼ˆå½“CSVä¸­çš„URLæ²¡æœ‰åŸŸåæ—¶ä½¿ç”¨ï¼‰')
@click.option(
    '--csv', 'csv_file', required=True, type=click.Path(exists=True),
    help='CSVæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«è¦æ£€æµ‹çš„URLåˆ—è¡¨')
@click.option(
    '--log', '-l', default='',
    help='æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š./logs/url404_YYYYMMDD_HHMMSS.logï¼‰')
@click.option(
    '--html', default='./url404_result.html',
    help='HTMLæŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤ï¼š./url404_result.htmlï¼‰')
@click.option(
    '--concurrency', '-c', default=5, type=int,
    help='å¹¶å‘æ•°é‡ï¼ˆé»˜è®¤ï¼š5ï¼‰')
@click.option(
    '--dingding-webhook',
    default='https://oapi.dingtalk.com/robot/send?access_token='
            'ce631c399761d21df6460018238a6fd22c237e3feb7021c580f34967c9a6e951',
    help='é’‰é’‰æœºå™¨äººWebhook URLï¼ˆé»˜è®¤å·²é…ç½®ï¼‰')
@click.option(
    '--dingding-secret',
    default='SECdc9d0205aebf46618039a4bf770cb69ed87173bc7270cead292136c1'
            '4287708f',
    help='é’‰é’‰æœºå™¨äººç­¾åå¯†é’¥ï¼ˆé»˜è®¤å·²é…ç½®ï¼‰')
@click.option(
    '--no-dingding', is_flag=True, default=False,
    help='ç¦ç”¨é’‰é’‰é€šçŸ¥ï¼ˆè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰')
@click.option(
    '--timeout', default=30000, type=int,
    help='é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼Œé»˜è®¤ï¼š30000ï¼‰')
def url404(host, csv_file, log, html, concurrency,
           dingding_webhook, dingding_secret, no_dingding, timeout):
    """URL 404/500é”™è¯¯æ£€æµ‹å·¥å…·

    ä»CSVæ–‡ä»¶è¯»å–URLåˆ—è¡¨å¹¶æ£€æµ‹çŠ¶æ€ç ã€‚CSVæ–‡ä»¶åº”åŒ…å«ä»¥ä¸‹åˆ—ï¼š

    \b
    - url: é¡µé¢URLï¼ˆå¯ä»¥æ˜¯å®Œæ•´URLæˆ–ç›¸å¯¹è·¯å¾„ï¼‰
    - name: URLåç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæ ‡è¯†ï¼‰

    ç¤ºä¾‹ï¼š

    \b
    cptools url404 -h http://www.cafepress.com \\
        --csv test_10.csv -l log.log --html url404_result.html

    \b
    cptools url404 --host http://example.com \\
        --csv urls.csv -c 10
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼Œè‡ªåŠ¨ç”ŸæˆåŸºäºæ—¶é—´æˆ³çš„æ–‡ä»¶å
    if not log:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log = f'./logs/url404_{timestamp}.log'
        # ç¡®ä¿ logs ç›®å½•å­˜åœ¨
        Path('./logs').mkdir(parents=True, exist_ok=True)

    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(log)

    logger.info("=" * 80)
    logger.info("å¼€å§‹æ‰§è¡ŒURL 404æ£€æµ‹ä»»åŠ¡")
    logger.info(f"ä¸»æœºåœ°å€: {host}")
    logger.info(f"CSVæ–‡ä»¶: {csv_file}")
    logger.info(f"å¹¶å‘æ•°: {concurrency}")
    logger.info(f"è¶…æ—¶æ—¶é—´: {timeout}ms")
    logger.info("=" * 80)

    # æ£€æŸ¥Playwrightæ˜¯å¦å·²å®‰è£…
    try:
        from playwright.async_api import async_playwright  # noqa: F401
    except ImportError:
        logger.error("Playwrightæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright")
        logger.error("ç„¶åè¿è¡Œ: playwright install chromium")
        sys.exit(1)

    # è¯»å–CSVæ–‡ä»¶
    urls = read_csv_urls(csv_file, logger)
    if not urls:
        logger.error("CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
        sys.exit(1)

    logger.info(f"ä»CSVæ–‡ä»¶ä¸­è¯»å–åˆ° {len(urls)} ä¸ªURL")

    # åˆ é™¤æ—§çš„HTMLæŠ¥å‘Š
    html_path = Path(html)
    if html_path.exists():
        logger.info(f"åˆ é™¤æ—§çš„HTMLæŠ¥å‘Š: {html_path}")
        html_path.unlink()

    # æ‰§è¡Œæ£€æµ‹ä»»åŠ¡
    start_time = datetime.now()
    results = asyncio.run(
        run_url404_tasks(
            urls=urls,
            host=host,
            concurrency=concurrency,
            timeout=timeout,
            logger=logger
        )
    )
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    # ç»Ÿè®¡ç»“æœ
    total = len(results)
    success = sum(1 for r in results if r.get('status_code') and 200 <= r.get('status_code') < 400)
    error_404 = sum(1 for r in results if r.get('status_code') == 404)
    error_500 = sum(1 for r in results if r.get('status_code') and r.get('status_code') >= 500)
    other_errors = total - success - error_404 - error_500

    logger.info("=" * 80)
    logger.info("URL 404æ£€æµ‹ä»»åŠ¡å®Œæˆ")
    logger.info(f"æ€»æ•°: {total}")
    logger.info(f"æˆåŠŸ(2xx-3xx): {success}")
    logger.info(f"404é”™è¯¯: {error_404}")
    logger.info(f"500é”™è¯¯: {error_500}")
    logger.info(f"å…¶ä»–é”™è¯¯: {other_errors}")
    logger.info(f"è€—æ—¶: {duration:.2f}ç§’")
    logger.info("=" * 80)

    # ç”ŸæˆHTMLæŠ¥å‘Š
    try:
        generate_url404_html_report(results, html, title="URL 404æ£€æµ‹æŠ¥å‘Š")
        logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html}")

        # è‡ªåŠ¨åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š
        try:
            html_abs_path = Path(html).absolute()
            webbrowser.open(f'file://{html_abs_path}')
            logger.info("å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨å¤±è´¥: {str(e)}")
    except Exception as e:
        logger.error(f"ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {str(e)}")

    # å‘é€é’‰é’‰é€šçŸ¥
    if dingding_webhook and not no_dingding:
        try:
            notification_content = f"""### ğŸ” URL 404 Check Completed

**Time**: {start_time.strftime('%Y-%m-%d %H:%M:%S')}

**Results**: Total {total} | OK {success}âœ… | 404 {error_404}âš ï¸ | 500+ {error_500}âŒ

**Duration**: {duration:.2f}s

**Host**: `{host}`

**File**: `{csv_file}`
"""
            asyncio.run(
                send_dingding_notification(
                    dingding_webhook,
                    "URL 404 Check Completed",
                    notification_content,
                    secret=dingding_secret
                )
            )
            logger.info("é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
        except Exception as e:
            logger.error(f"å‘é€é’‰é’‰é€šçŸ¥å¤±è´¥: {str(e)}")
    elif no_dingding:
        logger.info("å·²ç¦ç”¨é’‰é’‰é€šçŸ¥ï¼ˆ--no-dingdingï¼‰")

    # å¦‚æœæœ‰å¤±è´¥çš„ä»»åŠ¡ï¼Œä»¥éé›¶çŠ¶æ€ç é€€å‡º
    if error_404 + error_500 + other_errors > 0:
        sys.exit(1)


def read_csv_urls(csv_file: str, logger) -> List[Dict]:
    """è¯»å–CSVæ–‡ä»¶ä¸­çš„URLåˆ—è¡¨

    æ”¯æŒçš„åˆ—åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ï¼š
    - url/URL: URLåœ°å€ï¼ˆå¿…éœ€ï¼‰
    - name/PRODUCT_ID: URLåç§°ï¼ˆå¯é€‰ï¼‰
    """
    urls = []

    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            if not reader.fieldnames:
                logger.error("CSVæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                return []

            # åˆ›å»ºåˆ—åæ˜ å°„ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            fieldnames_lower = {
                name.lower(): name for name in reader.fieldnames
            }

            # æŸ¥æ‰¾URLåˆ—
            url_column = None
            for possible_name in ['url', 'URL']:
                if possible_name.lower() in fieldnames_lower:
                    url_column = fieldnames_lower[possible_name.lower()]
                    break

            if not url_column:
                logger.error(
                    f"CSVæ–‡ä»¶å¿…é¡»åŒ…å«'url'æˆ–'URL'åˆ—ï¼Œ"
                    f"å½“å‰åˆ—: {', '.join(reader.fieldnames)}"
                )
                return []

            # æŸ¥æ‰¾åç§°åˆ—ï¼ˆä¼˜å…ˆçº§ï¼šname > PRODUCT_IDï¼‰
            name_column = None
            for possible_name in ['name', 'PRODUCT_ID', 'product_id',
                                  'title', 'TITLE']:
                if possible_name.lower() in fieldnames_lower:
                    name_column = fieldnames_lower[possible_name.lower()]
                    break

            logger.info(
                f"ä½¿ç”¨åˆ—: URL='{url_column}', "
                f"NAME='{name_column or '(è‡ªåŠ¨ç”Ÿæˆ)'}'")

            for idx, row in enumerate(reader, 1):
                url = row.get(url_column, '').strip()
                if not url:
                    logger.warning(f"ç¬¬{idx}è¡Œ: URLä¸ºç©ºï¼Œè·³è¿‡")
                    continue

                # è·å–åç§°
                if name_column:
                    name = (row.get(name_column, '').strip() or
                            f'url-{idx}')
                else:
                    name = f'url-{idx}'

                urls.append({
                    'url': url,
                    'name': name,
                    'index': idx
                })

    except Exception as e:
        logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        return []

    return urls


async def run_url404_tasks(
    urls: List[Dict],
    host: str,
    concurrency: int,
    timeout: int,
    logger
) -> List[Dict]:
    """è¿è¡ŒURLæ£€æµ‹ä»»åŠ¡"""
    results = []

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        try:
            launch_args = [
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-setuid-sandbox',
                '--disable-gpu',
                '--disable-software-rasterizer',
                '--disable-extensions',
                '--disable-background-networking',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-breakpad',
                '--disable-client-side-phishing-detection',
                '--disable-component-update',
                '--disable-default-apps',
                '--disable-domain-reliability',
                '--disable-features=AudioServiceOutOfProcess',
                '--disable-hang-monitor',
                '--disable-ipc-flooding-protection',
                '--disable-notifications',
                '--disable-offer-store-unmasked-wallet-cards',
                '--disable-popup-blocking',
                '--disable-print-preview',
                '--disable-prompt-on-repost',
                '--disable-renderer-backgrounding',
                '--disable-sync',
                '--disable-translate',
                '--metrics-recording-only',
                '--no-first-run',
                '--mute-audio',
                '--safebrowsing-disable-auto-update',
                '--enable-automation',
                '--password-store=basic',
                '--use-mock-keychain',
            ]

            browser = await p.chromium.launch(
                headless=True,
                args=launch_args,
                chromium_sandbox=False,
            )
            logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        except Exception as e:
            logger.error(f"å¯åŠ¨æµè§ˆå™¨å¤±è´¥: {str(e)}")
            logger.error("è¯·ç¡®ä¿å·²å®‰è£…Playwrightæµè§ˆå™¨: playwright install chromium")
            return []

        try:
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(concurrency)

            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = []
            for url_info in urls:
                task = check_single_url(
                    browser=browser,
                    url_info=url_info,
                    host=host,
                    timeout=timeout,
                    semaphore=semaphore,
                    logger=logger
                )
                tasks.append(task)

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†å¼‚å¸¸ç»“æœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    processed_results.append({
                        'url': urls[i]['url'],
                        'name': urls[i]['name'],
                        'status_code': None,
                        'status_text': 'Exception',
                        'error': str(result)
                    })
                else:
                    processed_results.append(result)

            results = processed_results

        finally:
            await browser.close()
            logger.info("æµè§ˆå™¨å·²å…³é—­")

    return results


async def check_single_url(
    browser: Browser,
    url_info: Dict,
    host: str,
    timeout: int,
    semaphore: asyncio.Semaphore,
    logger
) -> Dict:
    """æ£€æµ‹å•ä¸ªURLçš„çŠ¶æ€ç """
    url = url_info['url']
    name = url_info['name']
    index = url_info['index']

    async with semaphore:
        # æ„å»ºå®Œæ•´URL
        full_url = build_full_url(url, host)

        logger.info(f"[{index}] å¼€å§‹æ£€æµ‹: {full_url}")

        page = None
        context = None
        try:
            # éšæœºå»¶è¿Ÿï¼ˆæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼‰
            delay = random.uniform(1.0, 2.5)
            logger.debug(f"[{index}] éšæœºå»¶è¿Ÿ {delay:.2f} ç§’")
            await asyncio.sleep(delay)

            # åˆ›å»ºä¸Šä¸‹æ–‡
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent=(
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                    'AppleWebKit/537.36 (KHTML, like Gecko) '
                    'Chrome/120.0.0.0 Safari/537.36'
                ),
                locale='en-US',
                ignore_https_errors=True,
            )

            page = await context.new_page()

            # è®¾ç½®è¶…æ—¶
            page.set_default_navigation_timeout(timeout)
            page.set_default_timeout(timeout)

            # è®¿é—®é¡µé¢å¹¶è·å–å“åº”
            resp = await page.goto(full_url, wait_until='domcontentloaded')

            # è·å–çŠ¶æ€ç 
            status_code = resp.status if resp else None
            status_text = resp.status_text if resp else 'No Response'

            # åˆ¤æ–­çŠ¶æ€
            if status_code is None:
                error_msg = "æ— æ³•è·å–å“åº”"
                logger.warning(f"[{index}] {error_msg}: {full_url}")
            elif status_code == 404:
                error_msg = "é¡µé¢ä¸å­˜åœ¨(404)"
                logger.warning(f"[{index}] {error_msg}: {full_url}")
            elif status_code >= 500:
                error_msg = f"æœåŠ¡å™¨é”™è¯¯({status_code})"
                logger.error(f"[{index}] {error_msg}: {full_url}")
            elif status_code >= 400:
                error_msg = f"å®¢æˆ·ç«¯é”™è¯¯({status_code})"
                logger.warning(f"[{index}] {error_msg}: {full_url}")
            else:
                error_msg = ""
                logger.info(f"[{index}] æ£€æµ‹æˆåŠŸ [{status_code}]: {full_url}")

            await context.close()

            return {
                'url': full_url,
                'name': name,
                'status_code': status_code,
                'status_text': status_text,
                'error': error_msg
            }

        except Exception as e:
            error_msg = str(e)
            logger.error(f"[{index}] æ£€æµ‹å¤±è´¥: {full_url} - {error_msg}")

            if context:
                try:
                    await context.close()
                except Exception:
                    pass

            return {
                'url': full_url,
                'name': name,
                'status_code': None,
                'status_text': 'Error',
                'error': error_msg
            }


def build_full_url(url: str, host: str) -> str:
    """æ„å»ºå®Œæ•´URL

    å¦‚æœURLå·²ç»åŒ…å«åŸŸåï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨æä¾›çš„host
    """
    url = url.strip()

    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å®Œæ•´URL
    parsed = urlparse(url)
    if parsed.scheme and parsed.netloc:
        return url

    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œä¸hostç»„åˆ
    if not url.startswith('/'):
        url = '/' + url

    return urljoin(host, url)

