"""æˆªå±å‘½ä»¤å®ç°"""
import click
import asyncio
import csv
from pathlib import Path
from urllib.parse import urlparse, urljoin
from datetime import datetime
from typing import List, Dict, Optional
import sys

from playwright.async_api import async_playwright, Browser, Page
from cptools.utils.logger import setup_logger
from cptools.utils.html_report import generate_html_report
from cptools.utils.dingding import send_dingding_notification


@click.command()
@click.option('--host', '-h', required=True, help='é»˜è®¤ä¸»æœºåœ°å€ï¼ˆå½“CSVä¸­çš„URLæ²¡æœ‰åŸŸåæ—¶ä½¿ç”¨ï¼‰')
@click.option('--csv', 'csv_file', required=True, type=click.Path(exists=True), help='CSVæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«è¦æˆªå›¾çš„URLåˆ—è¡¨')
@click.option('--output', '-o', default='./screenshots', help='æˆªå›¾ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ï¼š./screenshotsï¼‰')
@click.option('--log', '-l', default='./screenshot.log', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š./screenshot.logï¼‰')
@click.option('--html', default='./result.html', help='HTMLæŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤ï¼š./result.htmlï¼‰')
@click.option('--concurrency', '-c', default=5, type=int, help='å¹¶å‘æ•°é‡ï¼ˆé»˜è®¤ï¼š5ï¼‰')
@click.option('--dingding-webhook', default='', help='é’‰é’‰æœºå™¨äººWebhook URLï¼ˆå¯é€‰ï¼‰')
@click.option('--timeout', default=30000, type=int, help='é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼Œé»˜è®¤ï¼š30000ï¼‰')
@click.option('--width', default=1920, type=int, help='æµè§ˆå™¨çª—å£å®½åº¦ï¼ˆé»˜è®¤ï¼š1920ï¼‰')
@click.option('--height', default=1080, type=int, help='æµè§ˆå™¨çª—å£é«˜åº¦ï¼ˆé»˜è®¤ï¼š1080ï¼‰')
@click.option('--template', default='default', type=click.Choice(['default', 'terminal', 'minimal']), help='HTMLæŠ¥å‘Šæ¨¡æ¿ï¼ˆé»˜è®¤ï¼šdefaultï¼‰')
def screenshot(host, csv_file, output, log, html, concurrency, dingding_webhook, timeout, width, height, template):
    """ç½‘é¡µæˆªå±å·¥å…·
    
    ä»CSVæ–‡ä»¶è¯»å–URLåˆ—è¡¨å¹¶è¿›è¡Œæˆªå›¾ã€‚CSVæ–‡ä»¶åº”åŒ…å«ä»¥ä¸‹åˆ—ï¼š
    
    \b
    - url: é¡µé¢URLï¼ˆå¯ä»¥æ˜¯å®Œæ•´URLæˆ–ç›¸å¯¹è·¯å¾„ï¼‰
    - name: æˆªå›¾åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæ ‡è¯†ï¼‰
    
    ç¤ºä¾‹ï¼š
    
    \b
    cptools screenshot -h http://www.cafepress.com --csv data.csv -l log.log --html result.html
    
    \b
    cptools screenshot --host http://example.com --csv urls.csv --output ./imgs -c 10
    """
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(log)
    
    logger.info("=" * 80)
    logger.info("å¼€å§‹æ‰§è¡Œæˆªå±ä»»åŠ¡")
    logger.info(f"ä¸»æœºåœ°å€: {host}")
    logger.info(f"CSVæ–‡ä»¶: {csv_file}")
    logger.info(f"è¾“å‡ºç›®å½•: {output}")
    logger.info(f"å¹¶å‘æ•°: {concurrency}")
    logger.info(f"è¶…æ—¶æ—¶é—´: {timeout}ms")
    logger.info(f"çª—å£å¤§å°: {width}x{height}")
    logger.info(f"æŠ¥å‘Šæ¨¡æ¿: {template}")
    logger.info("=" * 80)
    
    # æ£€æŸ¥Playwrightæ˜¯å¦å·²å®‰è£…
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        logger.error("Playwrightæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright")
        logger.error("ç„¶åè¿è¡Œ: playwright install chromium")
        sys.exit(1)
    
    # è¯»å–CSVæ–‡ä»¶
    urls = read_csv_urls(csv_file, logger)
    if not urls:
        logger.error("CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
        sys.exit(1)
    
    logger.info(f"ä»CSVæ–‡ä»¶ä¸­è¯»å–åˆ° {len(urls)} ä¸ªURL")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ‰§è¡Œæˆªå›¾ä»»åŠ¡
    start_time = datetime.now()
    results = asyncio.run(
        run_screenshot_tasks(
            urls=urls,
            host=host,
            output_dir=output_dir,
            concurrency=concurrency,
            timeout=timeout,
            width=width,
            height=height,
            logger=logger
        )
    )
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    # ç»Ÿè®¡ç»“æœ
    total = len(results)
    success = sum(1 for r in results if r.get('status') == 'success')
    failed = total - success
    
    logger.info("=" * 80)
    logger.info("æˆªå±ä»»åŠ¡å®Œæˆ")
    logger.info(f"æ€»æ•°: {total}")
    logger.info(f"æˆåŠŸ: {success}")
    logger.info(f"å¤±è´¥: {failed}")
    logger.info(f"è€—æ—¶: {duration:.2f}ç§’")
    logger.info("=" * 80)
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    try:
        generate_html_report(results, html, title="æˆªå±æŠ¥å‘Š", template=template)
        logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html} (æ¨¡æ¿: {template})")
    except Exception as e:
        logger.error(f"ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    # å‘é€é’‰é’‰é€šçŸ¥
    if dingding_webhook:
        try:
            notification_content = f"""### ğŸ“¸ æˆªå±ä»»åŠ¡å®Œæˆ
            
**æ‰§è¡Œæ—¶é—´**: {start_time.strftime('%Y-%m-%d %H:%M:%S')}

**æ‰§è¡Œç»“æœ**:
- æ€»æ•°: {total}
- æˆåŠŸ: {success} âœ…
- å¤±è´¥: {failed} âŒ
- è€—æ—¶: {duration:.2f}ç§’

**ä¸»æœºåœ°å€**: {host}

**CSVæ–‡ä»¶**: {csv_file}
"""
            asyncio.run(
                send_dingding_notification(
                    dingding_webhook,
                    "æˆªå±ä»»åŠ¡å®Œæˆ",
                    notification_content
                )
            )
        except Exception as e:
            logger.error(f"å‘é€é’‰é’‰é€šçŸ¥å¤±è´¥: {str(e)}")
    
    # å¦‚æœæœ‰å¤±è´¥çš„ä»»åŠ¡ï¼Œä»¥éé›¶çŠ¶æ€ç é€€å‡º
    if failed > 0:
        sys.exit(1)


def read_csv_urls(csv_file: str, logger) -> List[Dict]:
    """è¯»å–CSVæ–‡ä»¶ä¸­çš„URLåˆ—è¡¨
    
    æ”¯æŒçš„åˆ—åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ï¼š
    - url/URL: URLåœ°å€ï¼ˆå¿…éœ€ï¼‰
    - name/PRODUCT_ID: æˆªå›¾åç§°ï¼ˆå¯é€‰ï¼‰
    """
    urls = []
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            if not reader.fieldnames:
                logger.error("CSVæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                return []
            
            # åˆ›å»ºåˆ—åæ˜ å°„ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            fieldnames_lower = {name.lower(): name for name in reader.fieldnames}
            
            # æŸ¥æ‰¾URLåˆ—
            url_column = None
            for possible_name in ['url', 'URL']:
                if possible_name.lower() in fieldnames_lower:
                    url_column = fieldnames_lower[possible_name.lower()]
                    break
            
            if not url_column:
                logger.error(f"CSVæ–‡ä»¶å¿…é¡»åŒ…å«'url'æˆ–'URL'åˆ—ï¼Œå½“å‰åˆ—: {', '.join(reader.fieldnames)}")
                return []
            
            # æŸ¥æ‰¾åç§°åˆ—ï¼ˆä¼˜å…ˆçº§ï¼šname > PRODUCT_IDï¼‰
            name_column = None
            for possible_name in ['name', 'PRODUCT_ID', 'product_id', 'title', 'TITLE']:
                if possible_name.lower() in fieldnames_lower:
                    name_column = fieldnames_lower[possible_name.lower()]
                    break
            
            logger.info(f"ä½¿ç”¨åˆ—: URL='{url_column}', NAME='{name_column or '(è‡ªåŠ¨ç”Ÿæˆ)'}'")
            
            for idx, row in enumerate(reader, 1):
                url = row.get(url_column, '').strip()
                if not url:
                    logger.warning(f"ç¬¬{idx}è¡Œ: URLä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # è·å–åç§°
                if name_column:
                    name = row.get(name_column, '').strip() or f'screenshot-{idx}'
                else:
                    name = f'screenshot-{idx}'
                
                urls.append({
                    'url': url,
                    'name': name,
                    'index': idx
                })
        
    except Exception as e:
        logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        return []
    
    return urls


async def run_screenshot_tasks(
    urls: List[Dict],
    host: str,
    output_dir: Path,
    concurrency: int,
    timeout: int,
    width: int,
    height: int,
    logger
) -> List[Dict]:
    """è¿è¡Œæˆªå›¾ä»»åŠ¡"""
    results = []
    
    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        try:
            browser = await p.chromium.launch(headless=True)
            logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        except Exception as e:
            logger.error(f"å¯åŠ¨æµè§ˆå™¨å¤±è´¥: {str(e)}")
            logger.error("è¯·ç¡®ä¿å·²å®‰è£…Playwrightæµè§ˆå™¨: playwright install chromium")
            return []
        
        try:
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(concurrency)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = []
            for url_info in urls:
                task = screenshot_single_page(
                    browser=browser,
                    url_info=url_info,
                    host=host,
                    output_dir=output_dir,
                    timeout=timeout,
                    width=width,
                    height=height,
                    semaphore=semaphore,
                    logger=logger
                )
                tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    processed_results.append({
                        'url': urls[i]['url'],
                        'name': urls[i]['name'],
                        'status': 'failed',
                        'error': str(result)
                    })
                else:
                    processed_results.append(result)
            
            results = processed_results
            
        finally:
            await browser.close()
            logger.info("æµè§ˆå™¨å·²å…³é—­")
    
    return results


async def screenshot_single_page(
    browser: Browser,
    url_info: Dict,
    host: str,
    output_dir: Path,
    timeout: int,
    width: int,
    height: int,
    semaphore: asyncio.Semaphore,
    logger
) -> Dict:
    """æˆªå–å•ä¸ªé¡µé¢"""
    url = url_info['url']
    name = url_info['name']
    index = url_info['index']
    
    async with semaphore:
        # æ„å»ºå®Œæ•´URL
        full_url = build_full_url(url, host)
        
        logger.info(f"[{index}] å¼€å§‹æˆªå›¾: {full_url}")
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_name = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name or f'screenshot-{index}'
        filename = f"{safe_name}_{timestamp}.png"
        screenshot_path = output_dir / filename
        
        page = None
        try:
            # åˆ›å»ºæ–°é¡µé¢
            context = await browser.new_context(
                viewport={'width': width, 'height': height},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = await context.new_page()
            
            # å¯¼èˆªåˆ°é¡µé¢
            await page.goto(full_url, timeout=timeout, wait_until='networkidle')
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿é¡µé¢å®Œå…¨æ¸²æŸ“
            await asyncio.sleep(1)
            
            # æˆªå›¾
            await page.screenshot(path=str(screenshot_path), full_page=True)
            
            logger.info(f"[{index}] æˆªå›¾æˆåŠŸ: {full_url}")
            
            await context.close()
            
            return {
                'url': full_url,
                'name': name,
                'screenshot_path': str(screenshot_path),
                'status': 'success',
                'error': ''
            }
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"[{index}] æˆªå›¾å¤±è´¥: {full_url} - {error_msg}")
            
            if page:
                try:
                    await page.context.close()
                except:
                    pass
            
            return {
                'url': full_url,
                'name': name,
                'screenshot_path': '',
                'status': 'failed',
                'error': error_msg
            }


def build_full_url(url: str, host: str) -> str:
    """æ„å»ºå®Œæ•´URL
    
    å¦‚æœURLå·²ç»åŒ…å«åŸŸåï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨æä¾›çš„host
    """
    url = url.strip()
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å®Œæ•´URL
    parsed = urlparse(url)
    if parsed.scheme and parsed.netloc:
        return url
    
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œä¸hostç»„åˆ
    if not url.startswith('/'):
        url = '/' + url
    
    return urljoin(host, url)

